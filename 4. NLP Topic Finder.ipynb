{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1082efd7",
   "metadata": {},
   "source": [
    "# NLP - Finding Topics of Restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f68f7",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37135e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python Stuff\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#PySpark Stuff\n",
    "from pyspark.sql.functions import isnan, when, count, col, avg\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, BooleanType, DateType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e74615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .master('yarn') \\\n",
    "  .appName('spark-bigquery-demo') \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c575b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"big-data-yelp\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e200c4",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f41031",
   "metadata": {},
   "source": [
    "### Loading Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebcdfe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'red-formula-339716:gfds.yelp_business_basicdata'\n",
    "df_b = spark.read.format('bigquery').option('table', table).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75214d68",
   "metadata": {},
   "source": [
    "#### Flitering Restaurants and Food"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202bc803",
   "metadata": {},
   "source": [
    "We will be doing restuarants in Ohio only for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "190ff53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = df_b.filter(df_b.state.like(\"OH\")).filter(df_b.categories.like(\"%Restaurants%\")).drop('int64_field_0', 'categories','review_count', 'address','city','postal_code', 'name','latitude', 'longitude', 'stars', 'is_open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "768e7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c480d",
   "metadata": {},
   "source": [
    "### Loading Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b9fb1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_re = 'red-formula-339716:gfds.yelp_review'\n",
    "df_re = spark.read.format('bigquery').option('table', table_re).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df52b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = df_re.drop('cool','funny','useful', 'date', 'review_id', 'compliment_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae5717",
   "metadata": {},
   "source": [
    "### Joining and Group Reviews by Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da4175b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfj = df_r.join(df_re, ['business_id'], \"inner\").drop('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c56a627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+----+\n",
      "|business_id|state|stars|text|\n",
      "+-----------+-----+-----+----+\n",
      "|          0|    0|    0|   0|\n",
      "+-----------+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for null values\n",
    "dfj.select([count(when(isnan(c), c)).alias(c) for c in dfj.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db96652",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_re, df_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca71b578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:====================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         business_id|                text|\n",
      "+--------------------+--------------------+\n",
      "|--_nBudPOb1lNRgKf...|I would have neve...|\n",
      "|-QOl03c2B22yi_On0...|Came in traveling...|\n",
      "|-Qos8tv2j6lj9uMxV...|Hot Tots\n",
      "\n",
      "   So m...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dff = dfj.groupby(\"business_id\").agg(F.concat_ws(\", \", F.collect_list(dfj.text)).alias('text'))\n",
    "dff.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "338b8dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4377"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fce0ba",
   "metadata": {},
   "source": [
    "## NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ca6d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these to use nltk on GCP\n",
    "#! pip install --upgrade nltk\n",
    "#! python -m nltk.downloader punkt\n",
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d57859d",
   "metadata": {},
   "source": [
    "Got insporation from: Distributed Topic Modelling using Spark NLP and Spark MLLib(LDA)   \n",
    "https://medium.com/analytics-vidhya/distributed-topic-modelling-using-spark-nlp-and-spark-mllib-lda-6db3f06a4da3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a75b63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark NLP\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "931c47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"shrink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64eaaf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c04382f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unwanted characters and garbage\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0b2eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.extend(['pass', 'way', 'go', 'one', 'on', 'would', 'going', 'dd', 'yeah', 'lady', 'although', 'place', 'area', 'another', 'goes', 'got', 'im', 'even', 'getting', 'that', 'dai', 'maybe', 'wonder', 'hope', 'went', 'want', 'door', 'still', 'could', 'include', 'hope', 'make', 'theyr', 'back', 'almost', 'anything', 'across', 'need', 'thing', 'sure', 'ago', 'case', 'man', 'ivy', 'get', 'on', 'came', 'come', 'Harvard', 'ran', 'u', 'made', 'part', 'nothing', 'wont'])\n",
    "# remove stopwords\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"normalized\")\\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setStopWords(eng_stopwords) \\\n",
    "      .setCaseSensitive(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed9eb459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem the words to bring them to the root form.\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"stem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8a89853",
   "metadata": {},
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"stem\"]) \\\n",
    "    .setOutputCols([\"tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1c1a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pipeline = Pipeline(stages=[document_assembler, \\\n",
    "                                tokenizer,          \\\n",
    "                                normalizer,         \\\n",
    "                                stopwords_cleaner,  \\\n",
    "                                stemmer,            \\ \n",
    "                                finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "712a03c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model = nlp_pipeline.fit(dff)\n",
    "processed_df  = nlp_model.transform(dff)\n",
    "tokens_df1 = processed_df.select('business_id','tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3dff59b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         business_id|              tokens|\n",
      "+--------------------+--------------------+\n",
      "|--_nBudPOb1lNRgKf...|[food, great, car...|\n",
      "|-QOl03c2B22yi_On0...|[amaz, veget, sou...|\n",
      "|-Qos8tv2j6lj9uMxV...|[excit, try, glad...|\n",
      "|-RpXYkc-4WDM4iJhg...|[mean, usual, don...|\n",
      "|-YsHP7zcVcCWZ4wye...|[great, good, foo...|\n",
      "|-h7leD9mwrnV0JDa3...|[fridai, slow, te...|\n",
      "|-kgm5IIE54ncW6Ssa...|[whoa, stuff, ama...|\n",
      "|-mkYw8bj6B9CDqkm6...|[cool, spot, sunb...|\n",
      "|01bpUEQYIRXAfDThD...|[pleasant, atmosp...|\n",
      "|023THgSmMx1Q_FSUn...|[soda, size, paye...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokens_df1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a330c",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0dce5940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ea0f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=100, minDF=3.0)\n",
    "cv_model = cv.fit(tokens_df1)\n",
    "vectorized_tokens = cv_model.transform(tokens_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0921048d",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3299424c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 94:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -21224873.612972133\n",
      "The upper bound on perplexity: 4.375077270399759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "num_topics = 7\n",
    "lda = LDA(k=num_topics, maxIter=10)\n",
    "model = lda.fit(vectorized_tokens)\n",
    "ll = model.logLikelihood(vectorized_tokens)\n",
    "lp = model.logPerplexity(vectorized_tokens)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b63f65c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 95:>                                                         (0 + 7) / 7]\r",
      "\r",
      "[Stage 95:================>                                         (2 + 5) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "beer\n",
      "good\n",
      "great\n",
      "burger\n",
      "food\n",
      "bar\n",
      "time\n",
      "like\n",
      "servic\n",
      "order\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "food\n",
      "chicken\n",
      "good\n",
      "flavor\n",
      "restaur\n",
      "try\n",
      "great\n",
      "best\n",
      "love\n",
      "columbu\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "restaur\n",
      "servic\n",
      "food\n",
      "u\n",
      "order\n",
      "great\n",
      "good\n",
      "drink\n",
      "tabl\n",
      "dinner\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "order\n",
      "food\n",
      "like\n",
      "time\n",
      "good\n",
      "great\n",
      "locat\n",
      "servic\n",
      "realli\n",
      "dont\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "pizza\n",
      "order\n",
      "good\n",
      "great\n",
      "time\n",
      "like\n",
      "food\n",
      "servic\n",
      "sauc\n",
      "chees\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "food\n",
      "good\n",
      "order\n",
      "great\n",
      "servic\n",
      "time\n",
      "like\n",
      "restaur\n",
      "realli\n",
      "chicken\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "sandwich\n",
      "salad\n",
      "good\n",
      "side\n",
      "chees\n",
      "great\n",
      "love\n",
      "order\n",
      "delici\n",
      "lunch\n",
      "*************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# extract vocabulary from CountVectorizer\n",
    "vocab = cv_model.vocabulary\n",
    "topics = model.describeTopics()   \n",
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "109ac7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def topic_prediction(my_document):\n",
    "    input1 = [my_document]\n",
    "   # processed_string = nlp_model.transform(string_input)\n",
    "    transformed_string = cv_model.transform(input1)    \n",
    "    return lda_model.transform(transformed_string)\n",
    "topic_prediction_udf = udf(topic_prediction, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c925a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_predictions = model.transform(vectorized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f917a3e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 99:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|         business_id|              tokens|            features|   topicDistribution|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|--_nBudPOb1lNRgKf...|[quesadilla, fren...|(100,[0,1,2,3,4,5...|[4.58359064709114...|\n",
      "|-QOl03c2B22yi_On0...|[love, alwai, sto...|(100,[0,1,2,3,4,5...|[8.58646743685032...|\n",
      "|-Qos8tv2j6lj9uMxV...|[famili, great, e...|(100,[0,1,2,3,4,5...|[0.01066670297647...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "topic_predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8568ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = F.udf(lambda x: x.tolist().index(max(x)), IntegerType())\n",
    "topic_predictions = topic_predictions.withColumn(\"topicID\", max_index(\"topicDistribution\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df15dd90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 107:==================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|topicID|count|\n",
      "+-------+-----+\n",
      "|      1|  648|\n",
      "|      6|  139|\n",
      "|      3| 1512|\n",
      "|      5|  790|\n",
      "|      4|  559|\n",
      "|      2|  288|\n",
      "|      0|  441|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "topic_predictions.groupBy('topicID').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c5b883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_topics = topic_predictions.select('business_id', 'topicID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5ee56",
   "metadata": {},
   "source": [
    "### Saving to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d0bb6224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "table = 'red-formula-339716:gfds.b_id_with_topic'\n",
    "b_topics.write.format('bigquery') \\\n",
    "  .option('table', table) \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}